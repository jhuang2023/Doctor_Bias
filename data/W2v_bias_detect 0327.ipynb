{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>href</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>Ease_of_Appointment</th>\n",
       "      <th>Promptness</th>\n",
       "      <th>Courteous_Staff</th>\n",
       "      <th>Accurate_Diagnosis</th>\n",
       "      <th>Bedside_Manner</th>\n",
       "      <th>Spends_Time_with_Me</th>\n",
       "      <th>...</th>\n",
       "      <th>Advanced_Technology</th>\n",
       "      <th>Caring_Manner</th>\n",
       "      <th>Pain_Minimized</th>\n",
       "      <th>Satisfaction</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>crawl_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12949966</td>\n",
       "      <td>23507540</td>\n",
       "      <td>/dentists/Dr_Aarika_Anderson_Elter</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-11-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Great results</td>\n",
       "      <td>Dr. Anderson explained in detail my options fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-09-06 20:54:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12949967</td>\n",
       "      <td>27184431</td>\n",
       "      <td>/dentists/Dr_Aamir_Wahab</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2015-03-15</td>\n",
       "      <td>Brad s.</td>\n",
       "      <td>implant</td>\n",
       "      <td>Had an implant done and it was painless. I cou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-09-06 20:54:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12949968</td>\n",
       "      <td>26307282</td>\n",
       "      <td>/dentists/Dr_Aanal_Parikh</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-10-12</td>\n",
       "      <td>Bill johnson</td>\n",
       "      <td>Warning ..would not see this dentist</td>\n",
       "      <td>Do not go to this dentist office they scam peo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-09-06 20:54:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12949969</td>\n",
       "      <td>28904504</td>\n",
       "      <td>/dentists/Dr_Aaron_Aguilar</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2015-06-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Very thoughtful Dr.  Communicates/bedside mann...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-09-06 20:54:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12949970</td>\n",
       "      <td>28380953</td>\n",
       "      <td>/dentists/Dr_Aaron_D_Larsen</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dr larsen great others no</td>\n",
       "      <td>Would continue to see Dr.  Larsen however his ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-09-06 20:54:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id  review_id                                href  overall_rating  \\\n",
       "0  12949966   23507540  /dentists/Dr_Aarika_Anderson_Elter               5   \n",
       "1  12949967   27184431            /dentists/Dr_Aamir_Wahab               5   \n",
       "2  12949968   26307282           /dentists/Dr_Aanal_Parikh               1   \n",
       "3  12949969   28904504          /dentists/Dr_Aaron_Aguilar               5   \n",
       "4  12949970   28380953         /dentists/Dr_Aaron_D_Larsen               4   \n",
       "\n",
       "   Ease_of_Appointment  Promptness  Courteous_Staff  Accurate_Diagnosis  \\\n",
       "0                  4.0         NaN              NaN                 NaN   \n",
       "1                  5.0         NaN              NaN                 NaN   \n",
       "2                  1.0         NaN              NaN                 NaN   \n",
       "3                  5.0         NaN              NaN                 NaN   \n",
       "4                  3.0         NaN              NaN                 NaN   \n",
       "\n",
       "   Bedside_Manner  Spends_Time_with_Me         ...           \\\n",
       "0             NaN                  NaN         ...            \n",
       "1             NaN                  NaN         ...            \n",
       "2             NaN                  NaN         ...            \n",
       "3             NaN                  NaN         ...            \n",
       "4             NaN                  NaN         ...            \n",
       "\n",
       "   Advanced_Technology  Caring_Manner  Pain_Minimized  Satisfaction  \\\n",
       "0                  NaN            5.0             5.0           NaN   \n",
       "1                  5.0            5.0             5.0           5.0   \n",
       "2                  1.0            1.0             1.0           1.0   \n",
       "3                  5.0            5.0             5.0           5.0   \n",
       "4                  4.0            4.0             4.0           5.0   \n",
       "\n",
       "         date      reviewer                                 title  \\\n",
       "0  2012-11-16           NaN                        Great results    \n",
       "1  2015-03-15       Brad s.                               implant   \n",
       "2  2014-10-12  Bill johnson  Warning ..would not see this dentist   \n",
       "3  2015-06-11           NaN                                   NaN   \n",
       "4  2015-03-31           NaN             Dr larsen great others no   \n",
       "\n",
       "                                             content helpful_vote  \\\n",
       "0  Dr. Anderson explained in detail my options fo...          NaN   \n",
       "1  Had an implant done and it was painless. I cou...          NaN   \n",
       "2  Do not go to this dentist office they scam peo...          NaN   \n",
       "3  Very thoughtful Dr.  Communicates/bedside mann...          NaN   \n",
       "4  Would continue to see Dr.  Larsen however his ...          NaN   \n",
       "\n",
       "            crawl_date  \n",
       "0  2016-09-06 20:54:01  \n",
       "1  2016-09-06 20:54:10  \n",
       "2  2016-09-06 20:54:13  \n",
       "3  2016-09-06 20:54:14  \n",
       "4  2016-09-06 20:54:18  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_panel = pd.read_csv('./data/reviews_panel_t8.csv')\n",
    "review_panel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1796204, 23)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_panel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# combine the title and content as final mateiral\n",
    "review_panel_text = review_panel['title'].fillna('').astype(str) + \" \" + review_panel['content'].fillna('')\n",
    "sample_s = review_panel_text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process via spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Punctuation remove & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. u.k. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "he -PRON- PRON PRP nsubj xx True True\n",
      "his -PRON- ADJ PRP$ ROOT xxx True True\n",
      "him -PRON- PRON PRP dobj xxx True True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion. he his him')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'say', 'that', '-PRON-', 'do', 'not', 'buy', 'apple', 'for', 'her', '.', 'but', 'I', 'buy', 'mine', '.']\n"
     ]
    }
   ],
   "source": [
    "# examples of lemmatization function\n",
    "words_to_keep= set(['he','his','him', 'she','her', 'I','my','me','mine'])\n",
    "def lemma(x):\n",
    "    if x.text in words_to_keep: \n",
    "        return x.text\n",
    "    elif x.text not in words_to_keep:\n",
    "        return x.lemma_\n",
    "    \n",
    "    \n",
    "a_doc = nlp(\"He said that you didn't buy apples for her. But I bought mine.\")  # feed in string or documents\n",
    "print([lemma(x) for x in a_doc])   # lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['great',\n",
       "  'result',\n",
       "  'anderson',\n",
       "  'explain',\n",
       "  'in',\n",
       "  'detail',\n",
       "  'my',\n",
       "  'option',\n",
       "  'for',\n",
       "  'the',\n",
       "  'repair',\n",
       "  'on',\n",
       "  'my',\n",
       "  'tooth',\n",
       "  'in',\n",
       "  'the',\n",
       "  'end',\n",
       "  'I',\n",
       "  'receive',\n",
       "  'great',\n",
       "  'result'],\n",
       " ['implant',\n",
       "  'have',\n",
       "  'an',\n",
       "  'implant',\n",
       "  'do',\n",
       "  'and',\n",
       "  '-PRON-',\n",
       "  'be',\n",
       "  'painless',\n",
       "  'I',\n",
       "  'could',\n",
       "  'not',\n",
       "  'afford',\n",
       "  '-PRON-',\n",
       "  'so',\n",
       "  'he',\n",
       "  'put',\n",
       "  'me',\n",
       "  'on',\n",
       "  'a',\n",
       "  'payment',\n",
       "  'plan',\n",
       "  'I',\n",
       "  'be',\n",
       "  'pleased',\n",
       "  'with',\n",
       "  'the',\n",
       "  'service',\n",
       "  'great'],\n",
       " ['warning',\n",
       "  'would',\n",
       "  'not',\n",
       "  'see',\n",
       "  'this',\n",
       "  'dentist',\n",
       "  'do',\n",
       "  'not',\n",
       "  'go',\n",
       "  'to',\n",
       "  'this',\n",
       "  'dentist',\n",
       "  'office',\n",
       "  '-PRON-',\n",
       "  'scam',\n",
       "  'people',\n",
       "  'and',\n",
       "  'do',\n",
       "  'not',\n",
       "  'pay',\n",
       "  'there',\n",
       "  'bill',\n",
       "  '-PRON-',\n",
       "  'snow',\n",
       "  'plow',\n",
       "  'for',\n",
       "  '-PRON-',\n",
       "  'and',\n",
       "  'when',\n",
       "  'the',\n",
       "  'season',\n",
       "  'be',\n",
       "  'over',\n",
       "  '-PRON-',\n",
       "  'would',\n",
       "  'not',\n",
       "  'pay',\n",
       "  '-PRON-',\n",
       "  'this',\n",
       "  'be',\n",
       "  'just',\n",
       "  'to',\n",
       "  'let',\n",
       "  '-PRON-',\n",
       "  'know',\n",
       "  'what',\n",
       "  'type',\n",
       "  'of',\n",
       "  'dentist',\n",
       "  '-PRON-',\n",
       "  'deal',\n",
       "  'with'],\n",
       " ['very', 'thoughtful', 'communicatesbedside', 'manner', 'great'],\n",
       " ['larsen',\n",
       "  'great',\n",
       "  'other',\n",
       "  'no',\n",
       "  'would',\n",
       "  'continue',\n",
       "  'to',\n",
       "  'see',\n",
       "  'larsen',\n",
       "  'however',\n",
       "  'his',\n",
       "  'front',\n",
       "  'desk',\n",
       "  'secretary',\n",
       "  'be',\n",
       "  'something',\n",
       "  'else',\n",
       "  '-PRON-',\n",
       "  'sadden',\n",
       "  'me',\n",
       "  'to',\n",
       "  'have',\n",
       "  'to',\n",
       "  'give',\n",
       "  'my',\n",
       "  'money',\n",
       "  'to',\n",
       "  'someone',\n",
       "  'else']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize and fitler documents; deal with punctuation(2h 30mins)\n",
    "terms_list=[]\n",
    "words_to_filter = set(['dr', 'doctor', 'oh'])\n",
    "words_to_keep = set(['he','his','him','she','her','I','my','me','mine'])\n",
    "def lemma(x):\n",
    "    if x.text in words_to_keep: \n",
    "        return x.text\n",
    "    elif x.text not in words_to_keep:\n",
    "        return x.lemma_\n",
    "\n",
    "for doc in sample_s:\n",
    "    doc_cleared = textacy.preprocess_text(doc, lowercase=False, no_punct=True)\n",
    "    doc_new = nlp(doc_cleared)\n",
    "    tokens=[lemma(x) for x in doc_new]  #to keep pronouns\n",
    "    tokens_filtered = [y for y in tokens if y not in words_to_filter]\n",
    "    terms_list.append(tokens_filtered)\n",
    "\n",
    "print(terms_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Spellcheck and correction (checker 1)\n",
    "- This method is contributed by a kaggler, CPMP, who posted it here: https://www.kaggle.com/cpmpml/spell-checker-using-word2vec/notebook \n",
    "- This method requires to download Google's word2vec: https://github.com/mmihaltz/word2vec-GoogleNews-vectors.\n",
    "- __This method is imperfect__, but it can fix most of misspellings in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import the GoogleNews word2vec model (4 mins)\n",
    "spell_check_model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz',\n",
    "                                                                    binary=True,encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Definition part 1\n",
    "googlenews_words = spell_check_model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(googlenews_words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "WORDS = w_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Definition part 2\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a5be111abe4f098f7b66c8ac987ac2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c71631560ef48eb91abc0cd3bd8e020"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cdb0ec59194a57afad22dc22996fbe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f0b69e063a4fd48f8d3576da24ae7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4500f5e448416faf089962b3be5e61"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0e208e045c4dc3820ce161a22c4cb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86619005f16e4937bf980d4d30c3ddaf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278ddf05de544a94b1ba2cc2cc830084"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['mee', 'my', 'my'],\n",
       " ['competant', 'helpfull', 'accomodate'],\n",
       " ['proffessional', 'professional', 'professional'],\n",
       " ['knowlegeable', 'knowledgeable', 'knowledgable'],\n",
       " ['courteous', 'curious', 'courteous'],\n",
       " ['smile', 'smiled'],\n",
       " ['pleasent', 'pleasant']]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examples of the spell-checker 1\n",
    "a=[['mee','mmy','myy'],\n",
    "   ['competant','helpfull','accomodat'],\n",
    "   ['proffessional','professionali','professionalhe'],\n",
    "   ['knowlegeable','knowleedgeable','knowledgable'],\n",
    "   ['curteous','curtious','courtious'],\n",
    "   ['smilei','smileand'],\n",
    "   ['pleasent','plesant']]\n",
    "\n",
    "for line in tqdm_notebook(a, desc='1st loop'):\n",
    "    for i,word in tqdm_notebook(enumerate(line), desc='2nd loop',leave=True):\n",
    "        line[i]=correction(word)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- This __negligence__ is due to the definition of function __candidates(word)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mee'}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "candidates('mee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mee'}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word='mee'\n",
    "known([word])\n",
    "#known(edits1(word))\n",
    "#known(edits2(word)) \n",
    "#[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mee'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'my'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'proffessional'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'professional'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('mee')  #failed\n",
    "correction('mmy')\n",
    "correction('proffessional')  #failed\n",
    "correction('professionali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186124"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyError",
     "evalue": "'mmy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-c7a8d0620002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mWORDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mee'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mWORDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mmy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'mmy'"
     ]
    }
   ],
   "source": [
    "WORDS['mee'] # this typo exists in GoogleNews model\n",
    "WORDS['mmy'] # this typo is new to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "856615"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyError",
     "evalue": "'professionali'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-fcbacaaff7d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mWORDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'proffessional'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mWORDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'professionali'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'professionali'"
     ]
    }
   ],
   "source": [
    "WORDS['proffessional'] # this typo exists in GoogleNews model\n",
    "WORDS['professionali'] # this typo is new to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-065994ab98c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterms_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-16c0ddf73312>\u001b[0m in \u001b[0;36mcorrection\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcorrection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m\"Most probable spelling correction for word.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-191-2ba873a97a7b>\u001b[0m in \u001b[0;36mcandidates\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"Generate possible spelling corrections for word.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medits2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mee'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-16c0ddf73312>\u001b[0m in \u001b[0;36mknown\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m\"The subset of `words` that appear in the dictionary of WORDS.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-16c0ddf73312>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m\"The subset of `words` that appear in the dictionary of WORDS.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# spellcheck the documents and correct it\n",
    "#for line in terms_list:\n",
    "    for i,word in enumerate(line):\n",
    "        line[i]=correction(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Spellcheck and correction (checker 2)\n",
    "- Python package: autocorrect 0.3.0 (GitHub: https://github.com/phatpiglet/autocorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48015c0862747b1a76ff2e61b293c37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e846a4bbc2a947c383e4b0ee43b6aeb7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bcd9ccd4a7433797c0b6ff9be7b422"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7337e565d7794fccbecdce6a0c2f5d75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caf406afa9e4d33a7d2fe2e029f8d98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7660e1f194641b5ba8fd36990f81e9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3774420cf64895be6df87e5e3145b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce35d9d88e1b40cb985f8027588e7a28"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['mee', 'may', 'may'],\n",
       " ['competent', 'helpful', 'accomodate'],\n",
       " ['professional', 'professional', 'professional'],\n",
       " ['knowledgeable', 'knowledgeable', 'knowledgable'],\n",
       " ['curteous', 'curious', 'courteous'],\n",
       " ['smile', 'smiled'],\n",
       " ['pleasant', 'pleasant']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examples of the spell-checker 2\n",
    "a=[['mee','mmy','myy'],\n",
    "   ['competant','helpfull','accomodat'],\n",
    "   ['proffessional','professionali','professionalhe'],\n",
    "   ['knowlegeable','knowleedgeable','knowledgable'],\n",
    "   ['curteous','curtious','courtious'],\n",
    "   ['smilei','smileand'],\n",
    "   ['pleasent','plesant']]\n",
    "\n",
    "for line in tqdm_notebook(a, desc='1st loop'):\n",
    "    for i,word in tqdm_notebook(enumerate(line), desc='2nd loop',leave=True):\n",
    "        line[i]=spell(word)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3ac9c4894942078349edba937ea114"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xiaowan/anaconda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/xiaowan/anaconda/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/Users/xiaowan/anaconda/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-221-9c8d5968d7d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1st loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/xiaowan/anaconda/lib/python3.6/site-packages/autocorrect/__init__.py\u001b[0m in \u001b[0;36mspell\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     candidates = (common([word]) or exact([word]) or known([word]) or\n\u001b[0;32m---> 23\u001b[0;31m                   \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble_typos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                   [word])\n\u001b[1;32m     25\u001b[0m     \u001b[0mcorrection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNLP_COUNTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiaowan/anaconda/lib/python3.6/site-packages/autocorrect/word.py\u001b[0m in \u001b[0;36mdouble_typos\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble_typos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m\"\"\"letter combinations two typos away from word\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         return {e2 for e1 in self.typos()\n\u001b[0m\u001b[1;32m     72\u001b[0m                 for e2 in Word(e1).typos()}\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiaowan/anaconda/lib/python3.6/site-packages/autocorrect/word.py\u001b[0m in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m\"\"\"letter combinations two typos away from word\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         return {e2 for e1 in self.typos()\n\u001b[0;32m---> 72\u001b[0;31m                 for e2 in Word(e1).typos()}\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# spellcheck the documents and correct spellings\n",
    "# this means will cost more time\n",
    "logfile=open(\"./data/logfile.txt\",\"w\")\n",
    "\n",
    "for line in tqdm_notebook(terms_list[:100], dynamic_ncols=True, file=logfile, mininterval=2):\n",
    "    for i,word in enumerate(line):\n",
    "        line[i]=spell(word)\n",
    "\n",
    "logfile.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b50b32b27d40e793d8e2306fa7d7c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this means will cost more space\n",
    "terms_newlist=[]\n",
    "for line in tqdm_notebook(terms_list[:100], dynamic_ncols=True, file=logfile, mininterval=2):\n",
    "    line_cleared = [spell(x) for x in line]\n",
    "    terms_newlist.append(line_cleared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(terms_newlist[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# spell-check the terms_list and save into terms_newlist\n",
    "def myfunc(line):\n",
    "    line_cleared = [spell(x) for x in line]\n",
    "    return line_cleared\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logfile=open('./log_file1.txt','w')\n",
    "    terms_newlist=[]\n",
    "    \n",
    "    pool = multiprocessing.Pool(processes=4)   \n",
    "    for line in tqdm(terms_list,dynamic_ncols=True,file=logfile,mininterval=1800):\n",
    "        terms_newlist.append(pool.apply(myfunc, args=(line,)))\n",
    "    \n",
    "    logfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Save & import trained corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save the corpus as a txt file (2 min)\n",
    "with open(\"./data/terms_list_0320.txt\",\"w\") as thefile:d\n",
    "    for item in terms_list:\n",
    "      thefile.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-923a355fca8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" '\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mterms_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# read in corpus (7 mins)\n",
    "with open('./data/terms_list.txt','r') as file:\n",
    "    terms_list=[]\n",
    "    for line in file.readlines():\n",
    "        tmp=[]\n",
    "        for i in line[1:-2].split(','):\n",
    "            tmp.append(i.strip(\" '\"))\n",
    "        terms_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1796204"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['great', 'result', 'anderson', 'explain', 'in', 'detail', 'my', 'option', 'for', 'the', 'repair', 'on', 'my', 'tooth', 'in', 'the', 'end', 'I', 'receive', 'great', 'result'], ['implant', 'have', 'an', 'implant', 'do', 'and', '-PRON-', 'be', 'painless', 'I', 'could', 'not', 'afford', '-PRON-', 'so', 'he', 'put', 'me', 'on', 'a', 'payment', 'plan', 'I', 'be', 'pleased', 'with', 'the', 'service', 'great'], ['warning', 'would', 'not', 'see', 'this', 'dentist', 'do', 'not', 'go', 'to', 'this', 'dentist', 'office', '-PRON-', 'scam', 'people', 'and', 'do', 'not', 'pay', 'there', 'bill', '-PRON-', 'snow', 'plow', 'for', '-PRON-', 'and', 'when', 'the', 'season', 'be', 'over', '-PRON-', 'would', 'not', 'pay', '-PRON-', 'this', 'be', 'just', 'to', 'let', '-PRON-', 'know', 'what', 'type', 'of', 'dentist', '-PRON-', 'deal', 'with'], ['very', 'thoughtful', 'communicatesbedside', 'manner', 'great'], ['larsen', 'great', 'other', 'no', 'would', 'continue', 'to', 'see', 'larsen', 'however', 'his', 'front', 'desk', 'secretary', 'be', 'something', 'else', '-PRON-', 'sadden', 'me', 'to', 'have', 'to', 'give', 'my', 'money', 'to', 'someone', 'else']]\n"
     ]
    }
   ],
   "source": [
    "len(terms_list)\n",
    "print(terms_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Build model via word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train word2vec on the corpus (10 mins)\n",
    "model = gensim.models.Word2Vec(sentences=terms_list, size=100, window=7, min_count=5, workers=4)\n",
    "# to save a trained model\n",
    "model.save(\"./data/model_doc_comments2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8985168 ,  0.21133737, -1.0174987 ,  0.23950161,  0.6535615 ,\n",
       "       -2.1597543 , -0.96719617, -1.2723322 ,  0.6952619 ,  0.52317786,\n",
       "        3.8957522 , -4.2797956 , -3.5484123 ,  2.9083505 , -2.0390408 ,\n",
       "        1.4697789 ,  2.7010856 , -2.316967  ,  1.357116  ,  2.9058228 ,\n",
       "       -1.5485703 , -2.5820878 ,  1.5781274 , -4.7464647 ,  2.5480347 ,\n",
       "       -1.0955589 , -4.0962763 ,  2.5019717 , -1.9741727 ,  2.3681483 ,\n",
       "        2.2892256 , -1.7966137 ,  2.341933  ,  1.9638792 ,  1.0589308 ,\n",
       "       -3.6643832 ,  0.8503319 ,  2.3559475 ,  0.8861464 ,  2.4579482 ,\n",
       "        0.09558363, -1.3544432 , -1.3810408 ,  3.4680386 , -2.2576418 ,\n",
       "       -1.45918   ,  1.1770133 ,  0.47717357,  0.82000923,  1.227406  ,\n",
       "       -3.9137058 , -0.06107468, -0.597307  , -2.9509046 , -1.9264892 ,\n",
       "       -0.3778871 , -1.1883132 ,  2.1589031 , -0.23421006,  0.4014843 ,\n",
       "       -1.801502  , -2.567079  , -1.50274   , -1.3262734 ,  1.2228625 ,\n",
       "       -3.5384681 ,  2.4423347 ,  1.5887536 ,  1.7367889 ,  0.56629264,\n",
       "       -2.976403  ,  1.7316047 ,  0.85862345, -4.0094047 , -1.3027035 ,\n",
       "       -0.02309173,  0.14540017, -1.4939944 ,  2.3036516 , -4.358884  ,\n",
       "        0.6274549 ,  1.6369541 , -1.5460306 ,  1.5804508 ,  2.087859  ,\n",
       "        0.091611  , -1.0865207 , -1.2028426 ,  2.2024171 , -0.512689  ,\n",
       "        1.8277788 ,  4.8530684 , -1.461438  , -4.029867  ,  1.0429525 ,\n",
       "        0.01837958,  2.701353  ,  0.3471363 , -2.8164046 , -0.8185738 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['I']  # numpy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaowan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.8985168 ,  0.21133737, -1.0174987 ,  0.23950161,  0.6535615 ,\n",
       "       -2.1597543 , -0.96719617, -1.2723322 ,  0.6952619 ,  0.52317786,\n",
       "        3.8957522 , -4.2797956 , -3.5484123 ,  2.9083505 , -2.0390408 ,\n",
       "        1.4697789 ,  2.7010856 , -2.316967  ,  1.357116  ,  2.9058228 ,\n",
       "       -1.5485703 , -2.5820878 ,  1.5781274 , -4.7464647 ,  2.5480347 ,\n",
       "       -1.0955589 , -4.0962763 ,  2.5019717 , -1.9741727 ,  2.3681483 ,\n",
       "        2.2892256 , -1.7966137 ,  2.341933  ,  1.9638792 ,  1.0589308 ,\n",
       "       -3.6643832 ,  0.8503319 ,  2.3559475 ,  0.8861464 ,  2.4579482 ,\n",
       "        0.09558363, -1.3544432 , -1.3810408 ,  3.4680386 , -2.2576418 ,\n",
       "       -1.45918   ,  1.1770133 ,  0.47717357,  0.82000923,  1.227406  ,\n",
       "       -3.9137058 , -0.06107468, -0.597307  , -2.9509046 , -1.9264892 ,\n",
       "       -0.3778871 , -1.1883132 ,  2.1589031 , -0.23421006,  0.4014843 ,\n",
       "       -1.801502  , -2.567079  , -1.50274   , -1.3262734 ,  1.2228625 ,\n",
       "       -3.5384681 ,  2.4423347 ,  1.5887536 ,  1.7367889 ,  0.56629264,\n",
       "       -2.976403  ,  1.7316047 ,  0.85862345, -4.0094047 , -1.3027035 ,\n",
       "       -0.02309173,  0.14540017, -1.4939944 ,  2.3036516 , -4.358884  ,\n",
       "        0.6274549 ,  1.6369541 , -1.5460306 ,  1.5804508 ,  2.087859  ,\n",
       "        0.091611  , -1.0865207 , -1.2028426 ,  2.2024171 , -0.512689  ,\n",
       "        1.8277788 ,  4.8530684 , -1.461438  , -4.029867  ,  1.0429525 ,\n",
       "        0.01837958,  2.701353  ,  0.3471363 , -2.8164046 , -0.8185738 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['I']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure the stereotype bias\n",
    "- via comparing comments on female and male physicians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "model = gensim.models.Word2Vec.load(\"./data/model_doc_comments1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build sets of gender words & attribute words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>man</th>\n",
       "      <th>woman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he</td>\n",
       "      <td>she</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>his</td>\n",
       "      <td>her</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>him</td>\n",
       "      <td>her</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>himself</td>\n",
       "      <td>herself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>man</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>men</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>boy</td>\n",
       "      <td>girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>guy</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>uncle</td>\n",
       "      <td>aunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>brother</td>\n",
       "      <td>sister</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        man    woman\n",
       "0        he      she\n",
       "1       his      her\n",
       "2       him      her\n",
       "3   himself  herself\n",
       "4       man    woman\n",
       "5       men    women\n",
       "6      male   female\n",
       "7       boy     girl\n",
       "8       guy      gal\n",
       "9     uncle     aunt\n",
       "10  brother   sister"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gender words collected\n",
    "Gender_words=pd.read_excel('./data/Gender_words.xlsx')\n",
    "Gender_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'him', 'himself', 'male', 'guy', 'he', 'man', 'boy', 'men', 'his'}\n",
      "{'she', 'women', 'her', 'gal', 'herself', 'girl', 'woman', 'female'}\n"
     ]
    }
   ],
   "source": [
    "# final gender word sets\n",
    "man_set= set(Gender_words.man)-{'uncle','brother'}  #n=9\n",
    "woman_set= set(Gender_words.woman)-{'aunt','sister'} #n=8\n",
    "print(man_set)\n",
    "print(woman_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction</th>\n",
       "      <th>technique</th>\n",
       "      <th>enviroment</th>\n",
       "      <th>administration</th>\n",
       "      <th>interpersonal</th>\n",
       "      <th>technical</th>\n",
       "      <th>enviromental</th>\n",
       "      <th>administrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empathy</td>\n",
       "      <td>outcome</td>\n",
       "      <td>facility</td>\n",
       "      <td>arrangemnet</td>\n",
       "      <td>responsive</td>\n",
       "      <td>expert</td>\n",
       "      <td>comfortable</td>\n",
       "      <td>timely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manner</td>\n",
       "      <td>result</td>\n",
       "      <td>atmosphere</td>\n",
       "      <td>timeliness</td>\n",
       "      <td>reliable</td>\n",
       "      <td>professional</td>\n",
       "      <td>smelly</td>\n",
       "      <td>organized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>responsiveness</td>\n",
       "      <td>expert</td>\n",
       "      <td>tangible</td>\n",
       "      <td>waiting</td>\n",
       "      <td>enthusiastic</td>\n",
       "      <td>competent</td>\n",
       "      <td>clean</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relationship</td>\n",
       "      <td>expertise</td>\n",
       "      <td>surrounding</td>\n",
       "      <td>appointment</td>\n",
       "      <td>supportive</td>\n",
       "      <td>knowledgeable</td>\n",
       "      <td>sterile</td>\n",
       "      <td>managed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interplay</td>\n",
       "      <td>professionalism</td>\n",
       "      <td>smell</td>\n",
       "      <td>operation</td>\n",
       "      <td>caring</td>\n",
       "      <td>qualified</td>\n",
       "      <td>decorative</td>\n",
       "      <td>efficient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>communication</td>\n",
       "      <td>competency</td>\n",
       "      <td>layout</td>\n",
       "      <td>admin</td>\n",
       "      <td>empathetic</td>\n",
       "      <td>skilled</td>\n",
       "      <td>clean</td>\n",
       "      <td>accessable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>attitude</td>\n",
       "      <td>competence</td>\n",
       "      <td>color</td>\n",
       "      <td>administration</td>\n",
       "      <td>interactive</td>\n",
       "      <td>standard</td>\n",
       "      <td>enviromental</td>\n",
       "      <td>costly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>behaviour</td>\n",
       "      <td>reliability</td>\n",
       "      <td>look</td>\n",
       "      <td>coordination</td>\n",
       "      <td>mutual</td>\n",
       "      <td>able</td>\n",
       "      <td>NaN</td>\n",
       "      <td>affordable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>behavior</td>\n",
       "      <td>assurance</td>\n",
       "      <td>decoration</td>\n",
       "      <td>organization</td>\n",
       "      <td>friendly</td>\n",
       "      <td>productive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>administrative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>interaction</td>\n",
       "      <td>knowledge</td>\n",
       "      <td>lighting</td>\n",
       "      <td>integration</td>\n",
       "      <td>pleasant</td>\n",
       "      <td>working</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mutuality</td>\n",
       "      <td>qualification</td>\n",
       "      <td>parking</td>\n",
       "      <td>support</td>\n",
       "      <td>behavioral</td>\n",
       "      <td>diagnostic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>trust</td>\n",
       "      <td>skill</td>\n",
       "      <td>temperature</td>\n",
       "      <td>furniture</td>\n",
       "      <td>encouraging</td>\n",
       "      <td>diagnosing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>friendship</td>\n",
       "      <td>ability</td>\n",
       "      <td>seating</td>\n",
       "      <td>management</td>\n",
       "      <td>understandable</td>\n",
       "      <td>trained</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>attention</td>\n",
       "      <td>standard</td>\n",
       "      <td>privacy</td>\n",
       "      <td>efficiency</td>\n",
       "      <td>concerned</td>\n",
       "      <td>efficacious</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>warmth</td>\n",
       "      <td>treatment</td>\n",
       "      <td>decoration</td>\n",
       "      <td>system</td>\n",
       "      <td>warming</td>\n",
       "      <td>therapeutic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>respect</td>\n",
       "      <td>diagnosis</td>\n",
       "      <td>location</td>\n",
       "      <td>accessibility</td>\n",
       "      <td>respectful</td>\n",
       "      <td>prognostic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kindness</td>\n",
       "      <td>diagnoses</td>\n",
       "      <td>surrounding</td>\n",
       "      <td>convenience</td>\n",
       "      <td>kind</td>\n",
       "      <td>educated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>willingness</td>\n",
       "      <td>improvement</td>\n",
       "      <td>instrument</td>\n",
       "      <td>cost</td>\n",
       "      <td>courteous</td>\n",
       "      <td>diagnostic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>courtesy</td>\n",
       "      <td>efficacy</td>\n",
       "      <td>blanket</td>\n",
       "      <td>continuity</td>\n",
       "      <td>compassionate</td>\n",
       "      <td>preventive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>compassion</td>\n",
       "      <td>therapy</td>\n",
       "      <td>lollipop</td>\n",
       "      <td>affordability</td>\n",
       "      <td>attentive</td>\n",
       "      <td>proficient</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rapport</td>\n",
       "      <td>prevention</td>\n",
       "      <td>tool</td>\n",
       "      <td>billing</td>\n",
       "      <td>amiable</td>\n",
       "      <td>experienced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>attentiveness</td>\n",
       "      <td>prognosis</td>\n",
       "      <td>enviroment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>courteous</td>\n",
       "      <td>helpful</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>amiability</td>\n",
       "      <td>alleviation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>personable</td>\n",
       "      <td>efficient</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>patience</td>\n",
       "      <td>education</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>warm</td>\n",
       "      <td>beneficial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>manner</td>\n",
       "      <td>triage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polite</td>\n",
       "      <td>efficacious</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>smiling</td>\n",
       "      <td>helpfulness</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>attentive</td>\n",
       "      <td>skillful</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>smile</td>\n",
       "      <td>proficiency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pleasant</td>\n",
       "      <td>precise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>handshake</td>\n",
       "      <td>experience</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>welcoming</td>\n",
       "      <td>scientific</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>grin</td>\n",
       "      <td>methodology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cordial</td>\n",
       "      <td>therapeutic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hug</td>\n",
       "      <td>execution</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cheerful</td>\n",
       "      <td>technical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>scowl</td>\n",
       "      <td>precision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>charming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>laughter</td>\n",
       "      <td>technique</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>delightful</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>expression</td>\n",
       "      <td>mastery</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>congenial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>smirk</td>\n",
       "      <td>pharmacology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>warming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tongue</td>\n",
       "      <td>method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hearty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>implementation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lively</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>interpersonal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       interaction         technique   enviroment  administration  \\\n",
       "0          empathy           outcome     facility     arrangemnet   \n",
       "1           manner            result   atmosphere      timeliness   \n",
       "2   responsiveness            expert     tangible         waiting   \n",
       "3     relationship         expertise  surrounding     appointment   \n",
       "4        interplay  professionalism         smell       operation   \n",
       "5    communication        competency       layout           admin   \n",
       "6         attitude        competence        color  administration   \n",
       "7        behaviour       reliability         look    coordination   \n",
       "8         behavior         assurance   decoration    organization   \n",
       "9      interaction         knowledge     lighting     integration   \n",
       "10       mutuality     qualification      parking         support   \n",
       "11           trust             skill  temperature       furniture   \n",
       "12      friendship           ability      seating      management   \n",
       "13       attention          standard      privacy      efficiency   \n",
       "14          warmth         treatment   decoration          system   \n",
       "15         respect         diagnosis     location   accessibility   \n",
       "16        kindness         diagnoses  surrounding     convenience   \n",
       "17     willingness       improvement   instrument            cost   \n",
       "18        courtesy          efficacy      blanket      continuity   \n",
       "19      compassion           therapy     lollipop   affordability   \n",
       "20         rapport        prevention         tool         billing   \n",
       "21   attentiveness         prognosis   enviroment             NaN   \n",
       "22      amiability       alleviation          NaN             NaN   \n",
       "23        patience         education          NaN             NaN   \n",
       "24          manner            triage          NaN             NaN   \n",
       "25         smiling       helpfulness          NaN             NaN   \n",
       "26           smile       proficiency          NaN             NaN   \n",
       "27       handshake        experience          NaN             NaN   \n",
       "28            grin       methodology          NaN             NaN   \n",
       "29             hug         execution          NaN             NaN   \n",
       "30           scowl         precision          NaN             NaN   \n",
       "31        laughter         technique          NaN             NaN   \n",
       "32      expression           mastery          NaN             NaN   \n",
       "33           smirk      pharmacology          NaN             NaN   \n",
       "34          tongue            method          NaN             NaN   \n",
       "35             NaN    implementation          NaN             NaN   \n",
       "36             NaN               NaN          NaN             NaN   \n",
       "\n",
       "     interpersonal      technical  enviromental  administrative  \n",
       "0       responsive         expert   comfortable          timely  \n",
       "1         reliable   professional        smelly       organized  \n",
       "2     enthusiastic      competent         clean           admin  \n",
       "3       supportive  knowledgeable       sterile         managed  \n",
       "4           caring      qualified    decorative       efficient  \n",
       "5       empathetic        skilled         clean      accessable  \n",
       "6      interactive       standard  enviromental          costly  \n",
       "7           mutual           able           NaN      affordable  \n",
       "8         friendly     productive           NaN  administrative  \n",
       "9         pleasant        working           NaN             NaN  \n",
       "10      behavioral     diagnostic           NaN             NaN  \n",
       "11     encouraging     diagnosing           NaN             NaN  \n",
       "12  understandable        trained           NaN             NaN  \n",
       "13       concerned    efficacious           NaN             NaN  \n",
       "14         warming    therapeutic           NaN             NaN  \n",
       "15      respectful     prognostic           NaN             NaN  \n",
       "16            kind       educated           NaN             NaN  \n",
       "17       courteous     diagnostic           NaN             NaN  \n",
       "18   compassionate     preventive           NaN             NaN  \n",
       "19       attentive     proficient           NaN             NaN  \n",
       "20         amiable    experienced           NaN             NaN  \n",
       "21       courteous        helpful           NaN             NaN  \n",
       "22      personable      efficient           NaN             NaN  \n",
       "23            warm     beneficial           NaN             NaN  \n",
       "24          polite    efficacious           NaN             NaN  \n",
       "25       attentive       skillful           NaN             NaN  \n",
       "26        pleasant        precise           NaN             NaN  \n",
       "27       welcoming     scientific           NaN             NaN  \n",
       "28         cordial    therapeutic           NaN             NaN  \n",
       "29        cheerful      technical           NaN             NaN  \n",
       "30        charming            NaN           NaN             NaN  \n",
       "31      delightful            NaN           NaN             NaN  \n",
       "32       congenial            NaN           NaN             NaN  \n",
       "33         warming            NaN           NaN             NaN  \n",
       "34          hearty            NaN           NaN             NaN  \n",
       "35          lively            NaN           NaN             NaN  \n",
       "36   interpersonal            NaN           NaN             NaN  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attribute words collected\n",
    "Vocabulary=pd.read_excel('./data/Vocabulary.xlsx', na_values=\"\")\n",
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'expression', 'behavior', 'mutuality', 'smiling', 'kindness', 'interplay', 'courtesy', 'attitude', 'trust', 'warmth', 'willingness', 'attentiveness', 'behaviour', 'patience', 'smile', 'manner', 'compassion', 'rapport', 'empathy', 'responsiveness', 'hug', 'friendship', 'interaction', 'attention', 'amiability', 'scowl', 'tongue', 'respect', 'handshake', 'relationship', 'smirk', 'laughter', 'communication', 'grin'}\n",
      "34 36 33 27\n"
     ]
    }
   ],
   "source": [
    "att_n_1= set(Vocabulary.interaction.dropna())\n",
    "att_n_2= set(Vocabulary.technique.dropna())\n",
    "att_adj_1= set(Vocabulary.interpersonal.dropna())\n",
    "att_adj_2= set(Vocabulary.technical.dropna())\n",
    "\n",
    "print(att_n_1)\n",
    "print(len(att_n_1),len(att_n_2),len(att_adj_1),len(att_adj_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep words existing in our model\n",
    "def set_filter(att_set):\n",
    "    new_set=[]\n",
    "    del_word=[]\n",
    "    for word in att_set:\n",
    "        try:\n",
    "            if model[word].all():\n",
    "                new_set.append(word)\n",
    "        except KeyError:\n",
    "            del_word.append(word)\n",
    "    print(del_word)\n",
    "    return set(new_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mutuality']\n",
      "['professionalism ']\n",
      "[]\n",
      "[]\n",
      "{'expression', 'behavior', 'smiling', 'kindness', 'interplay', 'attentiveness', 'courtesy', 'attitude', 'warmth', 'willingness', 'trust', 'behaviour', 'patience', 'smile', 'manner', 'compassion', 'rapport', 'empathy', 'responsiveness', 'hug', 'friendship', 'interaction', 'attention', 'amiability', 'scowl', 'tongue', 'respect', 'handshake', 'relationship', 'smirk', 'laughter', 'communication', 'grin'}\n",
      "33 35 33 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaowan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# final attribute word sets\n",
    "att_n_1=set_filter(att_n_1)\n",
    "att_n_2=set_filter(att_n_2)\n",
    "att_adj_1=set_filter(att_adj_1)\n",
    "att_adj_2=set_filter(att_adj_2)\n",
    "\n",
    "print(att_n_1)\n",
    "print(len(att_n_1),len(att_n_2),len(att_adj_1),len(att_adj_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: word analogies generated by model\n",
    "- __analogy__ 'he' + (adj.) - 'she' = ? |  __converse__ 'she' + (adj.) - 'he' = ?\n",
    "- __analogy__ 'his' + (noun) - 'her' = ? |  __converse__ 'her' + (noun) - 'his' = ?\n",
    "- __To be noticed: Results heavily relies on the choice of keyword pairs__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pattern 1: analogy  'he' + (adj.) - 'she' = ?  |  converse 'she' + (adj.) - 'he' = ?\n",
    "- keywords_adj_1 = {__'professional'__,'efficient','competent'}\n",
    "- keywords_adj_2 = {'nice','friendly',__'polite'__}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('proffesional', 0.6564347743988037),\n",
       " ('proffessional', 0.6406091451644897),\n",
       " ('personable', 0.6229191422462463),\n",
       " ('professionalhe', 0.6222789883613586),\n",
       " ('knowledgeable', 0.6085402369499207),\n",
       " ('humble', 0.608481228351593),\n",
       " ('professionali', 0.6083875894546509),\n",
       " ('competent', 0.6081501841545105),\n",
       " ('efficient', 0.6004852056503296),\n",
       " ('knowledgable', 0.5969569087028503)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('efficient', 0.6449151039123535),\n",
       " ('helpful', 0.6382687091827393),\n",
       " ('polite', 0.6301308274269104),\n",
       " ('proffesional', 0.623944878578186),\n",
       " ('attentive', 0.6222675442695618),\n",
       " ('respectful', 0.6202351450920105),\n",
       " ('warm', 0.6143782138824463),\n",
       " ('friendly', 0.6115367412567139),\n",
       " ('personable', 0.599821925163269),\n",
       " ('supportive', 0.5963608026504517)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# he+(adj.)-she\n",
    "model.wv.most_similar(positive=['he', 'professional'], negative=['she'], topn=10)\n",
    "# she+(adj.)-he\n",
    "model.wv.most_similar(positive=['she', 'professional'], negative=['he'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beneficial', 0.6459837555885315),\n",
       " ('conservative', 0.6394050121307373),\n",
       " ('innovative', 0.5897181034088135),\n",
       " ('economical', 0.5809915661811829),\n",
       " ('noninvasive', 0.572291910648346),\n",
       " ('prudent', 0.5711985230445862),\n",
       " ('prolotherapy', 0.5664175152778625),\n",
       " ('inexpensive', 0.5597268342971802),\n",
       " ('exacting', 0.5417249202728271),\n",
       " ('controversial', 0.5396667122840881)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('agreeable', 0.5745129585266113),\n",
       " ('economical', 0.5587084293365479),\n",
       " ('pragmatic', 0.54994797706604),\n",
       " ('beneficial', 0.5498460531234741),\n",
       " ('insightful', 0.5408653020858765),\n",
       " ('therapeutic', 0.5386779308319092),\n",
       " ('proactive', 0.5321661829948425),\n",
       " ('efficacious', 0.5267099738121033),\n",
       " ('openminded', 0.5205910801887512),\n",
       " ('astute', 0.5198585987091064)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# he+(adj.)-she\n",
    "model.wv.most_similar(positive=['he', 'effective'], negative=['she'], topn=10)\n",
    "# she+(adj.)-he\n",
    "model.wv.most_similar(positive=['she', 'effective'], negative=['he'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('skilled', 0.7663887143135071),\n",
       " ('capable', 0.7054327726364136),\n",
       " ('proficient', 0.7037885189056396),\n",
       " ('competant', 0.6754753589630127),\n",
       " ('talented', 0.673223078250885),\n",
       " ('experienced', 0.6638932228088379),\n",
       " ('skillful', 0.656440019607544),\n",
       " ('qualified', 0.6559319496154785),\n",
       " ('intelligent', 0.6537479758262634),\n",
       " ('knowledgeable', 0.6486888527870178)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('competant', 0.7423862218856812),\n",
       " ('capable', 0.6668056845664978),\n",
       " ('knowlegeable', 0.6394909620285034),\n",
       " ('proficient', 0.6360024213790894),\n",
       " ('knowledgable', 0.6337969303131104),\n",
       " ('knowledgeable', 0.6328122019767761),\n",
       " ('welltrain', 0.6277335286140442),\n",
       " ('experienced', 0.6254478096961975),\n",
       " ('knowlegable', 0.6198614835739136),\n",
       " ('engaged', 0.6171719431877136)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# he+(adj.)-she\n",
    "model.wv.most_similar(positive=['he', 'competent'], negative=['she'], topn=10)\n",
    "# she+(adj.)-he\n",
    "model.wv.most_similar(positive=['she', 'competent'], negative=['he'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('friendly', 0.6206178665161133),\n",
       " ('polite', 0.6170006394386292),\n",
       " ('likable', 0.6024885177612305),\n",
       " ('pleasant', 0.5870307683944702),\n",
       " ('friendy', 0.5755089521408081),\n",
       " ('likeable', 0.5656896829605103),\n",
       " ('pleasent', 0.5615097284317017),\n",
       " ('plesant', 0.5488872528076172),\n",
       " ('cool', 0.5443068742752075),\n",
       " ('charming', 0.543339192867279)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('sweet', 0.8070353865623474),\n",
       " ('lovely', 0.6870039701461792),\n",
       " ('friendly', 0.68165123462677),\n",
       " ('polite', 0.6418354511260986),\n",
       " ('pleasant', 0.623091995716095),\n",
       " ('pleasent', 0.6188926696777344),\n",
       " ('welcoming', 0.6035218834877014),\n",
       " ('unfriendly', 0.6007041335105896),\n",
       " ('snotty', 0.5943301320075989),\n",
       " ('plesant', 0.5931156277656555)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# he+(adj.)-she\n",
    "model.wv.most_similar(positive=['he', 'nice'], negative=['she'], topn=10)\n",
    "# she+(adj.)-he\n",
    "model.wv.most_similar(positive=['she', 'nice'], negative=['he'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('polite', 0.780290961265564),\n",
       " ('courteous', 0.7779586911201477),\n",
       " ('pleasant', 0.6985913515090942),\n",
       " ('personable', 0.6834683418273926),\n",
       " ('cordial', 0.6830505132675171),\n",
       " ('efficient', 0.6762321591377258),\n",
       " ('curteous', 0.6745314598083496),\n",
       " ('nice', 0.6646460294723511),\n",
       " ('curtious', 0.6569429039955139),\n",
       " ('courtious', 0.6483271718025208)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('polite', 0.7677793502807617),\n",
       " ('helpful', 0.7214754223823547),\n",
       " ('courteous', 0.7051953673362732),\n",
       " ('pleasant', 0.7012017965316772),\n",
       " ('sweet', 0.6971908807754517),\n",
       " ('welcoming', 0.6816627979278564),\n",
       " ('accomodat', 0.6815884113311768),\n",
       " ('cheerful', 0.6797686219215393),\n",
       " ('accommodating', 0.6655685901641846),\n",
       " ('cordial', 0.6591864824295044)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# he+(adj.)-she\n",
    "model.wv.most_similar(positive=['he', 'friendly'], negative=['she'], topn=10)\n",
    "# she+(adj.)-he\n",
    "model.wv.most_similar(positive=['she', 'friendly'], negative=['he'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('courteous', 0.777746319770813),\n",
       " ('friendly', 0.74969482421875),\n",
       " ('cordial', 0.7161503434181213),\n",
       " ('curteous', 0.7138438820838928),\n",
       " ('courtious', 0.6966575384140015),\n",
       " ('curtious', 0.6917126178741455),\n",
       " ('personable', 0.6851114630699158),\n",
       " ('efficient', 0.6846919059753418),\n",
       " ('helpful', 0.6555166244506836),\n",
       " ('helpfull', 0.650011420249939)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('friendly', 0.796964704990387),\n",
       " ('helpful', 0.7659982442855835),\n",
       " ('courteous', 0.7271215319633484),\n",
       " ('cordial', 0.7114821076393127),\n",
       " ('sweet', 0.7110629081726074),\n",
       " ('curtious', 0.706183135509491),\n",
       " ('curteous', 0.6957941055297852),\n",
       " ('accomodat', 0.6941994428634644),\n",
       " ('accommodating', 0.6795791983604431),\n",
       " ('courtious', 0.671298086643219)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# he+(adj.)-she\n",
    "model.wv.most_similar(positive=['he', 'polite'], negative=['she'], topn=10)\n",
    "# she+(adj.)-he\n",
    "model.wv.most_similar(positive=['she', 'polite'], negative=['he'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Parttern 2: analogy 'his' + (noun) - 'her' = ?  |  converse 'her' + (noun) - 'his' = ?\n",
    "- keywords_n_1 = {'technique','methodology','execution'}\n",
    "- keywords_n_2 = {'smile','grin',__'warm'__}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('technology', 0.60670006275177),\n",
       " ('cuttingedge', 0.5887786746025085),\n",
       " ('innovative', 0.5801389813423157),\n",
       " ('precision', 0.570105791091919),\n",
       " ('surgical', 0.5657679438591003),\n",
       " ('aesthetic', 0.5644391179084778),\n",
       " ('artistry', 0.5632181763648987),\n",
       " ('execution', 0.5563507676124573),\n",
       " ('technical', 0.5525886416435242),\n",
       " ('methodology', 0.544705867767334)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('technic', 0.5587255358695984),\n",
       " ('method', 0.5398416519165039),\n",
       " ('technology', 0.5327882766723633),\n",
       " ('tool', 0.5315686464309692),\n",
       " ('instrument', 0.46823790669441223),\n",
       " ('methodology', 0.4624430537223816),\n",
       " ('advancement', 0.46089014410972595),\n",
       " ('brava', 0.4550757110118866),\n",
       " ('vaseline', 0.4540991187095642),\n",
       " ('hrt', 0.45188942551612854)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# his+(noun)-her\n",
    "model.wv.most_similar(positive=['his', 'technique'], negative=['her'], topn=10)\n",
    "# her+(noun)-his\n",
    "model.wv.most_similar(positive=['her', 'technique'], negative=['his'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scientific', 0.6270068883895874),\n",
       " ('cuttingedge', 0.6237863302230835),\n",
       " ('innovative', 0.6234255433082581),\n",
       " ('execution', 0.6092877984046936),\n",
       " ('evidencebased', 0.5992555022239685),\n",
       " ('procedural', 0.5958627462387085),\n",
       " ('technique', 0.5914133191108704),\n",
       " ('mastery', 0.5861795544624329),\n",
       " ('pharmacology', 0.583117663860321),\n",
       " ('technical', 0.5739743113517761)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('method', 0.5421013236045837),\n",
       " ('therapeutic', 0.535122275352478),\n",
       " ('strategy', 0.5350595712661743),\n",
       " ('approach', 0.5251684188842773),\n",
       " ('commonsense', 0.5161302089691162),\n",
       " ('logic', 0.5129414796829224),\n",
       " ('theory', 0.5114564299583435),\n",
       " ('naturopathic', 0.5072319507598877),\n",
       " ('regimen', 0.5017905831336975),\n",
       " ('homeopathy', 0.4981175661087036)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# his+(noun)-her\n",
    "model.wv.most_similar(positive=['his', 'methodology'], negative=['her'], topn=10)\n",
    "# her+(noun)-his\n",
    "model.wv.most_similar(positive=['her', 'methodology'], negative=['his'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('precision', 0.6059291958808899),\n",
       " ('surgical', 0.5837683081626892),\n",
       " ('procedural', 0.5648508071899414),\n",
       " ('execute', 0.5634226202964783),\n",
       " ('bydons', 0.5609303116798401),\n",
       " ('perfectionism', 0.5564645528793335),\n",
       " ('technical', 0.5530896782875061),\n",
       " ('deductive', 0.5486582517623901),\n",
       " ('artistry', 0.5484542846679688),\n",
       " ('vickerys', 0.5405755043029785)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('implementation', 0.5497423410415649),\n",
       " ('roadmap', 0.4897775948047638),\n",
       " ('calculated', 0.4576040208339691),\n",
       " ('actioni', 0.44992777705192566),\n",
       " ('hulka', 0.44863227009773254),\n",
       " ('therapuetic', 0.44706565141677856),\n",
       " ('conception', 0.44655200839042664),\n",
       " ('gingivitis', 0.44056543707847595),\n",
       " ('wholebody', 0.43626290559768677),\n",
       " ('dimeanor', 0.4339507818222046)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# his+(noun)-her\n",
    "model.wv.most_similar(positive=['his', 'execution'], negative=['her'], topn=10)\n",
    "# her+(noun)-his\n",
    "model.wv.most_similar(positive=['her', 'execution'], negative=['his'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cheerful', 0.5321331024169922),\n",
       " ('smiling', 0.5262094736099243),\n",
       " ('handshake', 0.5197967290878296),\n",
       " ('smiley', 0.493060439825058),\n",
       " ('cheery', 0.479512095451355),\n",
       " ('poise', 0.4728557765483856),\n",
       " ('smilei', 0.4726208746433258),\n",
       " ('smileand', 0.470217764377594),\n",
       " ('grin', 0.46686071157455444),\n",
       " ('welcome', 0.4646042287349701)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('hug', 0.5498396158218384),\n",
       " ('handshake', 0.51720130443573),\n",
       " ('pantie', 0.5140137672424316),\n",
       " ('scowl', 0.5068621635437012),\n",
       " ('grin', 0.48976388573646545),\n",
       " ('shoelace', 0.48190537095069885),\n",
       " ('girl', 0.47115427255630493),\n",
       " ('smirk', 0.46735459566116333),\n",
       " ('blanket', 0.4635048806667328),\n",
       " ('lollipop', 0.4630123972892761)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# his+(noun)-her\n",
    "model.wv.most_similar(positive=['his', 'smile'], negative=['her'], topn=10)\n",
    "# her+(noun)-his\n",
    "model.wv.most_similar(positive=['her', 'smile'], negative=['his'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lets', 0.4965308606624603),\n",
       " ('wink', 0.48335111141204834),\n",
       " ('bluntness', 0.47686275839805603),\n",
       " ('scowl', 0.46949297189712524),\n",
       " ('laughter', 0.46827754378318787),\n",
       " ('heas', 0.4680287837982178),\n",
       " ('smirk', 0.45693308115005493),\n",
       " ('smile', 0.45322272181510925),\n",
       " ('expression', 0.4438091516494751),\n",
       " ('poker', 0.4429737329483032)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('smirk', 0.6086892485618591),\n",
       " ('scowl', 0.60369873046875),\n",
       " ('pantie', 0.5709898471832275),\n",
       " ('she', 0.5605303645133972),\n",
       " ('screaming', 0.5567564368247986),\n",
       " ('tongue', 0.5254310965538025),\n",
       " ('zit', 0.5244307518005371),\n",
       " ('flashlight', 0.5176804661750793),\n",
       " ('blister', 0.5069534778594971),\n",
       " ('pant', 0.5042190551757812)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# his+(noun)-her\n",
    "model.wv.most_similar(positive=['his', 'grin'], negative=['her'], topn=10)\n",
    "# her+(noun)-his\n",
    "model.wv.most_similar(positive=['her', 'grin'], negative=['his'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('professional', 0.6197702288627625),\n",
       " ('charming', 0.6069566011428833),\n",
       " ('efficient', 0.6004388332366943),\n",
       " ('cordial', 0.5991696119308472),\n",
       " ('delightful', 0.5822018384933472),\n",
       " ('congenial', 0.582179069519043),\n",
       " ('cheerful', 0.5816338658332825),\n",
       " ('friendly', 0.5798388123512268),\n",
       " ('respectful', 0.5742918252944946),\n",
       " ('charismatic', 0.5739561915397644)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('nurturing', 0.5317784547805786),\n",
       " ('sweet', 0.51926189661026),\n",
       " ('nurture', 0.5070083737373352),\n",
       " ('nonjudgemental', 0.4679553210735321),\n",
       " ('loving', 0.4642377495765686),\n",
       " ('cheery', 0.4581487774848938),\n",
       " ('bubbly', 0.4564359486103058),\n",
       " ('relatable', 0.4518079161643982),\n",
       " ('cheerful', 0.4480541944503784),\n",
       " ('warming', 0.4464818835258484)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# his+(noun)-her\n",
    "model.wv.most_similar(positive=['his', 'warm'], negative=['her'], topn=10)\n",
    "# her+(noun)-his\n",
    "model.wv.most_similar(positive=['her', 'warm'], negative=['his'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('greeting', 0.5082433223724365),\n",
       " ('hearty', 0.5034934878349304),\n",
       " ('smile', 0.4894843101501465),\n",
       " ('warmly', 0.4642738103866577),\n",
       " ('poise', 0.4488292932510376),\n",
       " ('charming', 0.4471122622489929),\n",
       " ('smilei', 0.43665194511413574),\n",
       " ('lively', 0.43590492010116577),\n",
       " ('cheery', 0.42895084619522095),\n",
       " ('quiet', 0.4270382225513458)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('greeting', 0.5603837370872498),\n",
       " ('hug', 0.5532957911491394),\n",
       " ('smile', 0.5467458367347717),\n",
       " ('blanket', 0.5412206649780273),\n",
       " ('robe', 0.5073226094245911),\n",
       " ('lollipop', 0.4984497129917145),\n",
       " ('gown', 0.4904882609844208),\n",
       " ('pantie', 0.4766647219657898),\n",
       " ('scowl', 0.4763096272945404),\n",
       " ('clipboard', 0.47618791460990906)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# his+(noun)-her\n",
    "model.wv.most_similar(positive=['his', 'handshake'], negative=['her'], topn=10)\n",
    "# her+(noun)-his\n",
    "model.wv.most_similar(positive=['her', 'handshake'], negative=['his'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: vector similarities calculated by model\n",
    "- __Compute cosine similarity between two words__\n",
    "- E.g. Similarity('woman', 'man')=0.585, similarity('woman', 'woman')=1\n",
    "- E.g. similarity('she','professional') - similarity('he','professional') < 0  \n",
    "==>  male physicians are more likely to be associated with 'professional' in patients' comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.20230704662320764\n",
      "-0.15150283317531602\n",
      "-0.05080421344789163\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('she','professional'))\n",
    "print(model.wv.similarity('he','professional'))\n",
    "\n",
    "print(model.wv.similarity('she','professional')-model.wv.similarity('he','professional'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05080421344789163\n",
      "0.0005303721403622508\n",
      "0.07739653979201196\n",
      "-------------\n",
      "0.04022910658313149\n",
      "0.01828594020133216\n",
      "0.006341864591320655\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('she','professional')-model.wv.similarity('he','professional'))\n",
    "print(model.wv.similarity('she','efficient')-model.wv.similarity('he','efficient'))\n",
    "print(model.wv.similarity('she','helpful')-model.wv.similarity('he','helpful'))\n",
    "print('-------------')\n",
    "print(model.wv.similarity('she','friendly')-model.wv.similarity('he','friendly'))\n",
    "print(model.wv.similarity('she','polite')-model.wv.similarity('he','polite'))\n",
    "print(model.wv.similarity('she','nice')-model.wv.similarity('he','nice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.16854096134784646\n",
      "-0.1182542978886263\n",
      "-0.33514627863349655\n",
      "-------------\n",
      "0.04540064853195455\n",
      "0.015162475385465113\n",
      "0.03154003349371577\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('her','technique')-model.wv.similarity('his','technique'))\n",
    "print(model.wv.similarity('her','methodology')-model.wv.similarity('his','methodology'))\n",
    "print(model.wv.similarity('her','execution')-model.wv.similarity('his','execution'))\n",
    "print('-------------')\n",
    "print(model.wv.similarity('her','smile')-model.wv.similarity('his','smile'))\n",
    "print(model.wv.similarity('her','handshake')-model.wv.similarity('his','handshake'))\n",
    "print(model.wv.similarity('her','grin')-model.wv.similarity('his','grin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- By comparison, we found that female physicians are relatively more likely to be associate with comforting behaviors while male physicians are more frequently to be judged according to their professional standards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcluate set_diff for each attribute word set\n",
    "- If __set_diff(set) < 0__\n",
    "==> this set of words are more likely to be associated with __female physicians__ in patients' comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the difference of mean similarity between each attribute word and two gender word sets\n",
    "def set_diff(att_set):\n",
    "    set_diff=[]\n",
    "    for att_word in att_set:\n",
    "        similarity_to_man_set = [model.wv.similarity(att_word, gender_word) for gender_word in man_set]\n",
    "        similarity_to_woman_set = [model.wv.similarity(att_word, gender_word) for gender_word in woman_set]\n",
    "        avg_sml_man = np.mean(similarity_to_man_set)  # mean similarity between one word and gender set\n",
    "        avg_sml_woman = np.mean(similarity_to_woman_set)\n",
    "\n",
    "        word_diff = avg_sml_man - avg_sml_woman\n",
    "        set_diff.append(word_diff)\n",
    "    return set_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0016594500767353837 0.04057073643120182 -0.00450266822901358 0.033617345083009155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.024827589523139654,\n",
       " -0.0492685056870774,\n",
       " -0.12639019131467882,\n",
       " 0.02014770284489957,\n",
       " -0.0028151667232429165,\n",
       " 0.01920039712405834,\n",
       " -0.015446332347368079,\n",
       " -0.03504298014533985,\n",
       " -0.03051393434320073,\n",
       " 0.0478100969079721,\n",
       " 0.09805954460912074,\n",
       " -0.09727288870162384,\n",
       " 0.02276570298050501,\n",
       " -0.038312325939864555,\n",
       " 0.024981612057599897,\n",
       " 0.010680235379252959,\n",
       " 0.03581622033355078,\n",
       " 0.012190248795722905,\n",
       " -0.002506463636056319,\n",
       " 0.00634225680298138,\n",
       " 0.05748664072698474,\n",
       " 0.01098021731532459,\n",
       " 0.02473904663105833,\n",
       " 0.05828247373004007,\n",
       " -0.0972871457751087,\n",
       " -0.01577976219175442,\n",
       " 0.028772509460524037,\n",
       " 0.030942373578036392,\n",
       " 0.016759788484555407,\n",
       " -0.056375191073041536,\n",
       " 0.035104887185893066,\n",
       " -0.033250516248703156,\n",
       " 0.00926518616985203]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the difference of similarities between att_set and two gender sets\n",
    "diff_n_1=set_diff(att_n_1)\n",
    "diff_n_2=set_diff(att_n_2)\n",
    "diff_adj_1=set_diff(att_adj_1)\n",
    "diff_adj_2=set_diff(att_adj_2)\n",
    "\n",
    "print(np.mean(diff_n_1), np.mean(diff_n_2), np.mean(diff_adj_1), np.mean(diff_adj_2))\n",
    "diff_n_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
